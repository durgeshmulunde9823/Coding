{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d4016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Logistic Regression\n",
      "Coefficients:\n",
      "     Feature  Coefficient\n",
      "0  Intercept    -0.407248\n",
      "1  Feature_1    -0.026732\n",
      "2  Feature_2    -0.068470\n",
      "3  Feature_3    -0.069753\n",
      "4  Feature_4     0.081558\n",
      "5  Feature_5     0.000857\n",
      "\n",
      "Model: SVM\n",
      "\n",
      "Model: Naive Bayes\n",
      "\n",
      "Model: Decision Tree\n",
      "\n",
      "Feature Importances:\n",
      "     Feature  Importance\n",
      "0  Feature_1    0.078409\n",
      "1  Feature_2    0.164855\n",
      "2  Feature_3    0.298841\n",
      "3  Feature_4    0.149405\n",
      "4  Feature_5    0.308490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\durgesh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "\n",
      "Feature Importances:\n",
      "     Feature  Importance\n",
      "0  Feature_1    0.184373\n",
      "1  Feature_2    0.202926\n",
      "2  Feature_3    0.221362\n",
      "3  Feature_4    0.199719\n",
      "4  Feature_5    0.191619\n",
      "\n",
      "Model: Linear Discriminant Analysis\n",
      "Coefficients:\n",
      "     Feature  Coefficient\n",
      "0  Intercept    -0.407418\n",
      "1  Feature_1    -0.028659\n",
      "2  Feature_2    -0.070093\n",
      "3  Feature_3    -0.071312\n",
      "4  Feature_4     0.083782\n",
      "5  Feature_5    -0.000696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\durgesh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: ANN\n",
      "\n",
      "Summary Table:\n",
      "                          Model  Accuracy  Precision  Recall  F1 Score\n",
      "0           Logistic Regression      0.60   0.360000    0.60  0.450000\n",
      "1                           SVM      0.71   0.804494    0.71  0.655770\n",
      "2                   Naive Bayes      0.62   0.668750    0.62  0.508392\n",
      "3                 Decision Tree      1.00   1.000000    1.00  1.000000\n",
      "4                 Random Forest      1.00   1.000000    1.00  1.000000\n",
      "5  Linear Discriminant Analysis      0.60   0.360000    0.60  0.450000\n",
      "6                           ANN      1.00   1.000000    1.00  1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\durgesh\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame(np.random.rand(100, 5), columns=['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5'])\n",
    "y = np.random.randint(2, size=100)  # Binary classification, change as needed\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Add a constant term to X for intercept\n",
    "X_with_intercept = sm.add_constant(X_scaled)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "    'ANN': MLPClassifier(max_iter=1000)  # Example, you might need to adjust parameters\n",
    "}\n",
    "\n",
    "# Fit models and print parameters\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_with_intercept, y)\n",
    "    \n",
    "    # Print parameters\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    \n",
    "    if hasattr(model, 'coef_'):  # Check if the model has coefficients\n",
    "        print(\"Coefficients:\")\n",
    "        coef_table = pd.DataFrame({'Feature': ['Intercept'] + list(X.columns), 'Coefficient': list(model.intercept_) + model.coef_.tolist()[0][1:]})\n",
    "        print(coef_table)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):  # Check if the model has feature importances\n",
    "        print(\"\\nFeature Importances:\")\n",
    "        feature_importance_table = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_[1:]})\n",
    "        print(feature_importance_table)\n",
    "    \n",
    "    if hasattr(model, 'pvalues'):  # Check if the model has p-values (for statsmodels)\n",
    "        print(\"\\nP-values:\")\n",
    "        p_values_table = pd.DataFrame({'Feature': X.columns, 'P-Value': model.pvalues})\n",
    "        print(p_values_table)\n",
    "\n",
    "    # KPIs\n",
    "    y_pred = model.predict(X_with_intercept)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    \n",
    "    kpi_table = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1 Score': [f1]\n",
    "    })\n",
    "    \n",
    "    results.append(kpi_table)\n",
    "\n",
    "# Display summary table\n",
    "summary_table = pd.concat(results, ignore_index=True)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efd3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
